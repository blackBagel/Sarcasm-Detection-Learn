{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sarcasm_detection-simple_models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IhCmO3fvV3lE",
        "1hBZQyhDXZqV",
        "LJY5hYrFXgrG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackBagel/Sarcasm-Detection-Learn/blob/main/sarcasm_detection_simple_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqllYLjHrLXf",
        "outputId": "0d5f21da-e3b5-4707-cde2-3fd1f795c269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "! pip install -U eli5 clean-text"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: eli5 in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already up-to-date: clean-text in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from eli5) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: graphviz in /usr/local/lib/python3.6/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: attrs>16.0.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (20.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.8.7)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from eli5) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.6/dist-packages (from eli5) (2.11.2)\n",
            "Requirement already satisfied, skipping upgrade: ftfy<6.0,>=5.8 in /usr/local/lib/python3.6/dist-packages (from clean-text) (5.8)\n",
            "Requirement already satisfied, skipping upgrade: emoji in /usr/local/lib/python3.6/dist-packages (from clean-text) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->eli5) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->eli5) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<6.0,>=5.8->clean-text) (0.2.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Azf7jp10cF"
      },
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import re\n",
        "from cleantext import clean\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing, decomposition, model_selection, pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "import eli5\n",
        "import pickle\n",
        "from time import perf_counter\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo17lJdKI2U3"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwDkTHa3IXTV",
        "outputId": "96e4853c-58ce-4a9e-ea92-562e283383c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "DRIVE_PATH = '/gdrive/My Drive/Reddit sarcasm'\n",
        "drive.mount('/gdrive')\n",
        "os.chdir(DRIVE_PATH)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_G25c4LINkx"
      },
      "source": [
        "DATA_COLUMNS = [\n",
        "    'label',\n",
        "    'comment',\n",
        "    'author',\n",
        "    'subreddit',\n",
        "    'score',\n",
        "    'ups',\n",
        "    'downs',\n",
        "    'date',\n",
        "    'created_utc',\n",
        "    'parent_comment'\n",
        "]\n",
        "\n",
        "full_comments_df = pd.read_csv(\"train-balanced-sarc.csv\", delimiter='\\t', names=DATA_COLUMNS)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqR8bwMI93PU"
      },
      "source": [
        "## Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmnNj35QuNTH"
      },
      "source": [
        "First, let's see the data distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki7ZyPskuWad",
        "outputId": "a9aa3d75-ab2a-4b67-887e-72b146d3b539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "full_comments_df.plot(kind='hist', y='label', title = 'Labels Distribution')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c0afcba20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp8nmrUIw1XY"
      },
      "source": [
        "The data is balanced with regard to the labels. Therefore, I'll use the accuracy metric to evaluate all future models.\n",
        "\n",
        "I prefer to evaluate all the models based primarily on accuracy. The data is well balanced and there is no actual buisiness problem i'm solving here, so there's not much of a need to specifically avoid either FPs or FNs.\n",
        "\n",
        "I will however keep the precision and recall scores of all models for future reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAad69rm_5O-"
      },
      "source": [
        "#### Train-Validation-Test Split "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ibCkqKJ88wY"
      },
      "source": [
        "The dataset is quite relatively large with about 1 million comments. For that reason, it is quite reasonable to use only 4% of the data as test and validation sets, since they'll still be 20k each.\n",
        "\n",
        "That way, we'll have lots of data to train on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX7ct9oLXFH2"
      },
      "source": [
        "TRAIN_TEST_RAND_STATE = 1\n",
        "VAL_TEST_RAND_STATE = 5\n",
        "TRAIN_TEST_SPLIT = 0.04\n",
        "VAL_TEST_SPLIT = 0.5\n",
        "\n",
        "comments_df_without_labels = full_comments_df.iloc[:, 1:]\n",
        "\n",
        "# We use the same train-test split to test all of the next models\n",
        "df_train, df_test, labels_train, labels_test = train_test_split(comments_df_without_labels,\n",
        "                                                                full_comments_df['label'],\n",
        "                                                                random_state=TRAIN_TEST_RAND_STATE,\n",
        "                                                                test_size=TRAIN_TEST_SPLIT,\n",
        "                                                                shuffle=True)\n",
        "\n",
        "df_val, df_test, labels_val, labels_test = train_test_split(df_test,\n",
        "                                                            labels_test,\n",
        "                                                            random_state=VAL_TEST_RAND_STATE,\n",
        "                                                            test_size=VAL_TEST_SPLIT,\n",
        "                                                            shuffle=True)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TgiVb7wNQjc"
      },
      "source": [
        "Let's make sure the validation and test set have similar distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSuOoIfwCRko",
        "outputId": "7a332aa1-1d1a-4089-d2b3-2f867592dde3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1,ncols=2,figsize=(8,4))\n",
        "\n",
        "labels_val.plot.hist(ax = axes[0], title='Validation Labels')\n",
        "labels_test.plot.hist(ax = axes[1], title='Test Labels')\n",
        "fig.tight_layout()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnUmivNMFlIC"
      },
      "source": [
        "#### Null handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaU1cGgkVTTr",
        "outputId": "a0f23e55-769f-468e-9f3d-515f9e54eac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(\"Nulls in training set:\")\n",
        "df_train.isnull().sum()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nulls in training set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "comment           51\n",
              "author             0\n",
              "subreddit          0\n",
              "score              0\n",
              "ups                0\n",
              "downs              0\n",
              "date               0\n",
              "created_utc        0\n",
              "parent_comment     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlTUZog0s7IZ"
      },
      "source": [
        "Since I don't want to divide and reassemble the data and the labels each time i apply a function to both, i'll simply create a function that applies row based functions to them and handles the dividing and reassembling for me"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBZwEjEYdxFj"
      },
      "source": [
        "def handle_row_dropping_functions(df, labels, row_dropping_funcs={}):\n",
        "  \"\"\"\n",
        "  Applies data munging functions that may drop rows in the dataset\n",
        "  This function makes sure the corresponding labels of each dropped row is deleted as well\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  df : pandas DataFrame\n",
        "      The dataset to apply the functions on\n",
        "  labels : pandas Series\n",
        "      The corresponding labels of the dataset\n",
        "  row_dropping_funcs : Dict\n",
        "      A dictionary containing functions as keys\n",
        "      and any parameters necessary for them (except the df!) as values\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  new_df\n",
        "      The recieved DF after being handled the fucntions\n",
        "  new_labels:\n",
        "      The labels for the df, without the dropped rows\n",
        "  \"\"\"\n",
        "\n",
        "  # Combine data and labels based on their index\n",
        "  new_df = df.copy()\n",
        "  new_labels = labels.copy()\n",
        "  new_df['label'] = new_labels\n",
        "  \n",
        "  # Apply row dropping functions\n",
        "  for row_dropping_func, func_params in row_dropping_funcs.items():\n",
        "    func_params['df'] = new_df\n",
        "    new_df = row_dropping_func(**func_params)\n",
        "  \n",
        "  # Return the updated datasets as they were before\n",
        "  new_labels = new_df['label']\n",
        "  new_df = new_df.drop(columns = ['label'])\n",
        "\n",
        "  return new_df, new_labels"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-cgrlu4OKxT"
      },
      "source": [
        "def handle_nulls(**kwargs):\n",
        "  \"\"\"\n",
        "  Returns the df without rows where some columns are null\n",
        "  Right now could have been done without a function,\n",
        "  but useful for encapsulating more complex null handling situations in the future\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  df : pandas DataFrame\n",
        "      The dataset to drop NaNs from\n",
        "  unfixable_columns : list\n",
        "      The columns that need to be dropped if empty (default is an empty list)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  new_df\n",
        "      The recieved DF but with rows without null columns\n",
        "  \"\"\"\n",
        "\n",
        "  new_df = kwargs['df'].copy()\n",
        "  new_df = new_df.dropna(subset = kwargs['unfixable_columns'])\n",
        "  return new_df"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbwwENfYdATn"
      },
      "source": [
        "def nan_handle(df, labels):\n",
        "  new_df = df.copy()\n",
        "  new_labels = labels.copy()\n",
        "\n",
        "  # If the value of these columns is null, drop the row \n",
        "  COLUMNS_TO_DROP_BY = ['comment']\n",
        "\n",
        "  row_dropping_funcs = {\n",
        "      handle_nulls: { 'unfixable_columns': COLUMNS_TO_DROP_BY }\n",
        "      }\n",
        "\n",
        "  new_df, new_labels = handle_row_dropping_functions(new_df, new_labels, row_dropping_funcs)\n",
        "  return new_df, new_labels"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeT_OluY8I3G"
      },
      "source": [
        "#### Text cleaning - Comments only\n",
        "\n",
        "For starters, i'll only explore the comments themselves to see how much sarcasm can be inferred from the text alone.\n",
        "\n",
        "Later on i'll start engineering more features from the rest of the data to see how much the context matters for inferring sarcasm with this data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPlExkPR9pfX"
      },
      "source": [
        "Let's look at a sample from the text to see what we're handling with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOqAE9M28NM_",
        "outputId": "c4c1e59a-809b-4640-a2b3-9812861bb247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for index, text in enumerate(df_train['comment'][35:40]):\n",
        "  print('Comment {}:\\n {}'.format(index+1, text))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comment 1:\n",
            " That Stormtrooper pumpkin looks awesome!\n",
            "Comment 2:\n",
            " Yeah, Barry will let this slide.\n",
            "Comment 3:\n",
            " Uh oh, Trigga Trey gonna get some revenge.\n",
            "Comment 4:\n",
            " fuck your opinion\n",
            "Comment 5:\n",
            " we know, but not to loud or the wackos will catch on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAv1n5EEVHLX"
      },
      "source": [
        "NLP_ENGINES = {\n",
        "    'simple' : spacy.load(\"en_core_web_sm\", disable=['tagger', 'parser', 'ner']),\n",
        "    'POSs_and_deps' : spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
        "}"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjy2NRjl3SST"
      },
      "source": [
        "def tokenize_and_clean_text(doc):\n",
        "  tokenized_processed_text = []\n",
        "  for token in doc: \n",
        "    cleaned_text = token.text.lower()\n",
        "    if token.is_currency:\n",
        "      cleaned_text = clean(cleaned_text, no_currency_symbols = True)\n",
        "    if token.like_email:\n",
        "      cleaned_text = clean(cleaned_text, no_emails = True)\n",
        "    if token.like_url:\n",
        "      cleaned_text = clean(cleaned_text, no_urls = True)\n",
        "    \n",
        "    tokenized_processed_text.append(cleaned_text)\n",
        "  \n",
        "  return tokenized_processed_text"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPs--zXAwhey"
      },
      "source": [
        "###### Simple text cleaning and tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weaHzGQSNi0V"
      },
      "source": [
        "def simple_single_text_processing(text):\n",
        "  # spaCy does not handle numbers without text well, so we'll clean them in  the beginning\n",
        "  clean(text, no_numbers=True)\n",
        "  \n",
        "  # spaCy will only handle contractions and will just tokenize the data in general\n",
        "  nlp_engine = NLP_ENGINES['simple']\n",
        "  doc = nlp_engine(text)\n",
        "\n",
        "  list_of_processed_tokens = tokenize_and_clean_text(doc)\n",
        "  return ' '.join(list_of_processed_tokens)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpuhzfTB2cQ-"
      },
      "source": [
        "def simple_text_processing(df, text_columns='comment'):\n",
        "  new_fd = df.copy()\n",
        "  for column in text_columns:\n",
        "    new_df['processed ' + column] = new_df[column].apply(lambda text: simple_single_text_processing(text))\n",
        "  return new_df"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYKucVjsIJsU"
      },
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd6zKdwTl-9H"
      },
      "source": [
        "##### More advanced text features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjnLrseohHsq"
      },
      "source": [
        "In order to process text for more advanced features, we'll use spacy's ready pipelines. But considering the fact we're dealing with 1M comments, it's worth trying to parallelize the workload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkE8mo9Y9ZV2"
      },
      "source": [
        "We'll do a few quick checks to see what are roughly the right parameters to run this parallelized processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGyJsSz2jxJ4",
        "outputId": "39116ed2-6978-401c-a703-7e9394d6ddd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "! lscpu | grep CPU\\(s\\)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "NUMA node0 CPU(s):   0,1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lfP97129pI_",
        "outputId": "08c01034-b627-41e0-e85e-9455ffaf9499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "# Since there's a few comments that are just numbers and no text, we'll convert all comments to string just in case\n",
        "comment_length = df_train['comment'].apply(lambda text: len(str(text)))\n",
        "comment_length.plot(kind='hist', y='comment length', title = 'Comments length distribution')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c28213eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwaRwKLz-6jB"
      },
      "source": [
        "Not surprisingly, most of the comments are pretty short. Let's see how many of them are actually huge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaLPZXvcB-ET",
        "outputId": "feed7206-65c8-4e81-f6be-c9bd0eb8e514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "comment_length.describe()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    970392.000000\n",
              "mean         56.691743\n",
              "std          62.433983\n",
              "min           1.000000\n",
              "25%          27.000000\n",
              "50%          46.000000\n",
              "75%          74.000000\n",
              "max       10000.000000\n",
              "Name: comment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrebEGAw_Kqm",
        "outputId": "bbe4ad64-7978-45bc-854e-fbfeb00b8163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "long_comment_mask = comment_length > 200\n",
        "print('Overall ratio of long comments: %.3f' % (long_comment_mask.mean()))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall ratio of long comments: 0.011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOFzlR_5FiK_"
      },
      "source": [
        "Well, because there are about 1% long comments, can use a large chunk size and pipe size since for every 100K comments i'll only get 1000 long comments, which is nothing compared to the amount of RAM i have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtikZFvRhlcx"
      },
      "source": [
        "def chunker(iterable, total_length, chunksize):\n",
        "    return [iterable[position: position + chunksize] for position in range(0, total_length, chunksize)]\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "    \"Flatten a list of lists to a combined list\"\n",
        "    return [item for sublist in list_of_lists for item in sublist]\n",
        "\n",
        "def process_chunk(texts):\n",
        "    new_columns = []\n",
        "    nlp_engine = NLP_ENGINES['POSs_and_deps']\n",
        "    \n",
        "    clean_texts = [clean(text, no_numbers=True) for text in texts]\n",
        "    for doc in nlp_engine.pipe(clean_texts, batch_size=20000):\n",
        "      features = {}\n",
        "\n",
        "      features['pos tags'] = ' '.join([token.tag_.lower() for token in doc])\n",
        "      features['word dependencies'] = ' '.join([token.dep_.lower() for token in doc])\n",
        "\n",
        "      # I'll also keep a simple tokenized version of the text since there's no reason to run spacy twice \n",
        "      features['processed comment'] = ' '.join(tokenize_and_clean_text(doc))\n",
        "\n",
        "      new_columns.append(features)\n",
        "\n",
        "    return new_columns\n",
        "\n",
        "def parallel_pos_and_dep_text_processing(texts, chunksize):\n",
        "    executor = Parallel(n_jobs=-1, backend='multiprocessing', prefer=\"processes\")\n",
        "    do = delayed(process_chunk)\n",
        "    tasks = [do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize)]\n",
        "    result = executor(tasks)\n",
        "    new_columns = flatten(result)\n",
        "    return pd.DataFrame(new_columns)\n",
        "\n",
        "def pos_and_dep_text_processing(df, text_columns='comment'):\n",
        "  new_df = df.copy()\n",
        "  for column in text_columns:\n",
        "    advanced_df_features = parallel_pos_and_dep_text_processing(new_df[column], chunksize=100000)\n",
        "    advanced_df_features.index = new_df.index\n",
        "    new_df = new_df.merge(advanced_df_features, how='left', left_index=True, right_index=True)\n",
        "\n",
        "  return new_df"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXblypr9OIN8"
      },
      "source": [
        "For chunk size of 100K and pipe size of 20K it took 16min and 22s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5cNehPtPlgU"
      },
      "source": [
        "def get_advanced_text_columns(df, processing_type='simple', from_where='scratch', file_path=None):\n",
        "  new_df = df.copy()\n",
        "  if file_path is not None:\n",
        "    if from_where == 'scratch':\n",
        "      if processing_type == 'simple':\n",
        "        new_df = simple_text_processing(new_df, text_columns=['comment'])\n",
        "      elif processing_type == 'pos_and_deps':\n",
        "        new_df = pos_and_dep_text_processing(new_df, text_columns=['comment'])\n",
        "        \n",
        "      new_df.to_csv(file_path)\n",
        "\n",
        "    elif from_where == 'file':\n",
        "      new_df_processed = pd.read_csv(file_path)\n",
        "      new_df_processed.index = new_df.index\n",
        "      new_df = new_df.merge(new_df_processed, how='left', left_index = True, right_index = True) \n",
        "    else:\n",
        "      print('You haven\\'t entered a valid file path')\n",
        "\n",
        "  return new_df"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUqsYI4MHXQo"
      },
      "source": [
        "def preprocess_df(df, labels, processing_type, extra_features_from, extra_features_file_path):\n",
        "  new_df = df.copy()\n",
        "  new_labels = labels.copy()  \n",
        "\n",
        "  new_df, new_labels = nan_handle(new_df, new_labels)\n",
        "\n",
        "  new_df = get_advanced_text_columns(new_df,\n",
        "                                     processing_type = processing_type,\n",
        "                                     from_where = extra_features_from,\n",
        "                                     file_path = extra_features_file_path)\n",
        "\n",
        "  return new_df, new_labels"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_uZf5JgHSk3"
      },
      "source": [
        "full_df_train_for_clf, labels_train_for_clf = preprocess_df(df_train, labels_train,\n",
        "                                                            processing_type = 'pos_and_deps',\n",
        "                                                            extra_features_from = 'file',\n",
        "                                                            extra_features_file_path = 'train_with_POSs_and_deps_rand_state_{}.csv'.format(TRAIN_TEST_RAND_STATE))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2KdKs6XXXHP",
        "outputId": "0b227c86-90a4-466b-9023-5df8a8a33d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for index, text in enumerate(full_df_train_for_clf['processed comment'][35:40]):\n",
        "  print('Comment {}:\\n {}'.format(index+1, text))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comment 1:\n",
            " that stormtrooper pumpkin looks awesome !\n",
            "Comment 2:\n",
            " yeah , barry will let this slide .\n",
            "Comment 3:\n",
            " uh oh , trigga trey gon na get some revenge .\n",
            "Comment 4:\n",
            " fuck your opinion\n",
            "Comment 5:\n",
            " we know , but not to loud or the wackos will catch on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEskcbTYJfqH"
      },
      "source": [
        "# Model Research Process - Comments Only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjhEt9PYEwHH"
      },
      "source": [
        "#### Preprocess the validation and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJNIPpKuJfUm"
      },
      "source": [
        "full_df_val_for_clf, labels_val_for_clf = preprocess_df(df_val, labels_val,\n",
        "                                                        processing_type = 'pos_and_deps',\n",
        "                                                        extra_features_from = 'file',\n",
        "                                                        extra_features_file_path = 'validation_with_POSs_and_deps_rand_state_{}.csv'.format(VAL_TEST_RAND_STATE))\n",
        "full_df_test_for_clf, labels_test_for_clf = preprocess_df(df_test, labels_test,\n",
        "                                                          processing_type = 'pos_and_deps',\n",
        "                                                          extra_features_from = 'file',\n",
        "                                                          extra_features_file_path = 'test_with_POSs_and_deps_rand_state_{}.csv'.format(VAL_TEST_RAND_STATE))"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFvpouHVJjG7"
      },
      "source": [
        "## We begin our research by trying out a basic logistic regression model on only a simple tokenized comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU4Bn3dF35cf"
      },
      "source": [
        "FEATURES_TO_CLASSIFY_BY = ['processed comment']\n",
        "df_train_for_clf = full_df_train_for_clf[FEATURES_TO_CLASSIFY_BY]\n",
        "df_val_for_clf = full_df_val_for_clf[FEATURES_TO_CLASSIFY_BY]\n",
        "df_test_for_clf = full_df_test_for_clf[FEATURES_TO_CLASSIFY_BY]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWfnvrvOAhO6"
      },
      "source": [
        "TEXTUAL_COLUMNS = 'processed comment'\n",
        "model_pipes_to_evaluate = {}\n",
        "MODELS_FOLDER = './models'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9IzfoDNMW12"
      },
      "source": [
        "def save_model_pipeline_and_info(model_pipe_and_info, file_name):\n",
        "  full_path = os.path.join(MODELS_FOLDER, file_name)\n",
        "  with open(full_path, 'wb') as model_pipeline_and_info_file:\n",
        "    pickle.dump(model_pipe_and_info, model_pipeline_and_info_file)\n",
        "            \n",
        "def get_model_pipeline_and_info(file_name):\n",
        "  full_path = os.path.join(MODELS_FOLDER, file_name)\n",
        "  if os.path.exists(full_path):\n",
        "    with open(full_path, 'rb') as model_pipeline_and_info_file:\n",
        "      model_pipe_and_info = pickle.load(model_pipeline_and_info_file)\n",
        "  else:\n",
        "      return None\n",
        "  return model_pipe_and_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhCmO3fvV3lE"
      },
      "source": [
        "### First, a relatively primitive count vectorizer of the comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "961AIbjXWYpT"
      },
      "source": [
        "# I'll assume a word that doesn't appear at least thrice, is probably too specific to care about\n",
        "ctv = CountVectorizer(min_df = 3)\n",
        "text_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', ctv, TEXTUAL_COLUMNS)\n",
        "    ])\n",
        "log_reg_ctv_clf = LogisticRegression(solver='saga', C=1, n_jobs=-1, verbose=False, max_iter=300)\n",
        "ctv_lr_pipeline = Pipeline([('counter', text_preprocessor), \n",
        "                            ('logitRegressor', log_reg_ctv_clf)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79CZ5F5UWZ_Z",
        "outputId": "8536523c-2ed6-4576-f6d2-d8606a4f874f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "%%time\n",
        "ctv_lr_pipeline.fit(df_train_for_clf, labels_train_for_clf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 6s, sys: 359 ms, total: 6min 6s\n",
            "Wall time: 4min 41s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('counter',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('text',\n",
              "                                                  CountVectorizer(analyzer='word',\n",
              "                                                                  binary=False,\n",
              "                                                                  decode_error='strict',\n",
              "                                                                  dtype=<class 'numpy.int64'>,\n",
              "                                                                  encoding='utf-8',\n",
              "                                                                  input='content',\n",
              "                                                                  lowercase=True,\n",
              "                                                                  max_df=1.0,\n",
              "                                                                  max_features=None,\n",
              "                                                                  min_df=3,\n",
              "                                                                  ngram_range=(1,\n",
              "                                                                               1...\n",
              "                                                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                                                  tokenizer=None,\n",
              "                                                                  vocabulary=None),\n",
              "                                                  'processed comment')],\n",
              "                                   verbose=False)),\n",
              "                ('logitRegressor',\n",
              "                 LogisticRegression(C=1, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=300,\n",
              "                                    multi_class='auto', n_jobs=-1, penalty='l2',\n",
              "                                    random_state=None, solver='saga',\n",
              "                                    tol=0.0001, verbose=False,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrADpSmwWzsb",
        "outputId": "6b4182fd-3b83-4336-8912-b3c297200e46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Logistic Regressor has a ready score function that returns the accuracy of the model on another dataset\n",
        "print (\"Training accuracy: %0.5f \" % ctv_lr_pipeline.score(df_train_for_clf, labels_train_for_clf))\n",
        "print (\"Validation accuracy: %0.5f \" % ctv_lr_pipeline.score(df_val_for_clf, labels_val_for_clf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.64346 \n",
            "Validation accuracy: 0.63717 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hBZQyhDXZqV"
      },
      "source": [
        "### We also test a basic tf-idf vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UkKmNJ8KytE"
      },
      "source": [
        "# We'll first try to create a simple baseline model\n",
        "# Our baseline model will be a logistic regression model, which we'll train on the unigram tf_idf vectors of the original comments text \n",
        "tfv = TfidfVectorizer(min_df = 3)\n",
        "text_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', tfv, TEXTUAL_COLUMNS)\n",
        "    ])\n",
        "lr_tfidf_clf = LogisticRegression(solver='saga', n_jobs=-1, verbose=False, max_iter=300)\n",
        "\n",
        "tfidf_lr_pipeline = Pipeline([('tf_idf', text_preprocessor), \n",
        "                                 ('logitRegressor', lr_tfidf_clf)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbcU2CtBlLGa",
        "outputId": "0530abdf-3e2e-4ad4-bae0-7e6252c241f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "%%time\n",
        "tfidf_lr_pipeline.fit(df_train_for_clf, labels_train_for_clf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 34.6 s, sys: 28 ms, total: 34.7 s\n",
            "Wall time: 34.7 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tf_idf',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('text',\n",
              "                                                  TfidfVectorizer(analyzer='word',\n",
              "                                                                  binary=False,\n",
              "                                                                  decode_error='strict',\n",
              "                                                                  dtype=<class 'numpy.float64'>,\n",
              "                                                                  encoding='utf-8',\n",
              "                                                                  input='content',\n",
              "                                                                  lowercase=True,\n",
              "                                                                  max_df=1.0,\n",
              "                                                                  max_features=None,\n",
              "                                                                  min_df=3,\n",
              "                                                                  ngram_range=(1,...\n",
              "                                                                  tokenizer=None,\n",
              "                                                                  use_idf=True,\n",
              "                                                                  vocabulary=None),\n",
              "                                                  'processed comment')],\n",
              "                                   verbose=False)),\n",
              "                ('logitRegressor',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=300,\n",
              "                                    multi_class='auto', n_jobs=-1, penalty='l2',\n",
              "                                    random_state=None, solver='saga',\n",
              "                                    tol=0.0001, verbose=False,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkaRSFgSlNh0",
        "outputId": "84e96ad4-1198-4109-c2d0-686d46fcd86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print (\"Training accuracy: %0.5f \" % tfidf_lr_pipeline.score(df_train_for_clf, labels_train_for_clf))\n",
        "print (\"Validation accuracy: %0.5f \" % tfidf_lr_pipeline.score(df_val_for_clf, labels_val_for_clf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.71713 \n",
            "Validation accuracy: 0.69158 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJY5hYrFXgrG"
      },
      "source": [
        "### We try a more sophisticated tf-idf vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDp5qv7mLQaq"
      },
      "source": [
        "# Now let's try to run the same model, but on a more sophisticated tf-idf \n",
        "# This vectorizor is trained on both unigrams and bigrams, and only considers words that were used at least twice\n",
        "tfv = TfidfVectorizer(ngram_range=(1, 2), min_df=3)\n",
        "text_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', tfv, TEXTUAL_COLUMNS)\n",
        "    ])\n",
        "lr_tfidf_clf = LogisticRegression(solver='saga', n_jobs=-1, verbose=False, max_iter=300)\n",
        "\n",
        "tfidf_lr_pipeline = Pipeline([('tf_idf', text_preprocessor), \n",
        "                              ('logitRegressor', lr_tfidf_clf)])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BmVM_nWVEjb",
        "outputId": "b3458c54-eefb-4efb-cfa3-4253455e9ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "%%time\n",
        "tfidf_lr_pipeline.fit(df_train_for_clf, labels_train_for_clf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 13s, sys: 850 ms, total: 1min 14s\n",
            "Wall time: 1min 14s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tf_idf',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('text',\n",
              "                                                  TfidfVectorizer(analyzer='word',\n",
              "                                                                  binary=False,\n",
              "                                                                  decode_error='strict',\n",
              "                                                                  dtype=<class 'numpy.float64'>,\n",
              "                                                                  encoding='utf-8',\n",
              "                                                                  input='content',\n",
              "                                                                  lowercase=True,\n",
              "                                                                  max_df=1.0,\n",
              "                                                                  max_features=None,\n",
              "                                                                  min_df=3,\n",
              "                                                                  ngram_range=(1,...\n",
              "                                                                  tokenizer=None,\n",
              "                                                                  use_idf=True,\n",
              "                                                                  vocabulary=None),\n",
              "                                                  'processed comment')],\n",
              "                                   verbose=False)),\n",
              "                ('logitRegressor',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=300,\n",
              "                                    multi_class='auto', n_jobs=-1, penalty='l2',\n",
              "                                    random_state=None, solver='saga',\n",
              "                                    tol=0.0001, verbose=False,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eefMkLZWVGDB",
        "outputId": "79aae77a-1b7c-45f9-b759-a9dae4bc3059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print (\"Training accuracy: %0.5f \" % tfidf_lr_pipeline.score(df_train_for_clf, labels_train_for_clf))\n",
        "print (\"Validation accuracy: %0.5f \" % tfidf_lr_pipeline.score(df_val_for_clf, labels_val_for_clf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.78497 \n",
            "Validation accuracy: 0.72373 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqOkWA0NVTBI",
        "outputId": "5bad085b-1513-4334-f096-0bd6bdfe647a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "# Because it was te best model we tried to understand better what affected his performance\n",
        "eli5.show_weights(estimator=tfidf_lr_pipeline.named_steps['logitRegressor'],\n",
        "                  vec=tfidf_lr_pipeline.named_steps['tf_idf'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "            \n",
              "                \n",
              "                \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=1\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +10.494\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__yes because\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 82.06%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +8.981\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__obviously\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 82.32%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +8.798\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__clearly\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 82.98%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +8.336\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__totally\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 82.98%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +8.335\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__yeah because\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 83.12%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +8.237\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__because\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 85.08%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +6.903\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__how dare\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 85.92%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +6.356\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__good thing\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.32%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +6.100\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__right because\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.91%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.725\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__gee\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.93%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.717\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__duh\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.12%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.594\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__but thought\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.18%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.561\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__fault\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.31%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.477\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__therefore\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.63%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.283\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__yeah fuck\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.90%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.116\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__shitlord\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.95%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.091\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__or anything\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +5.057\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__everyone knows\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.00%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 199169 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 88.11%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 187069 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 88.11%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -4.989\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__true but\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 87.53%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -5.341\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        text__iirc\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "            \n",
              "        \n",
              "\n",
              "        \n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grzSqsZ29FeO"
      },
      "source": [
        "So using tf-idf with bi-grams and a minimum df count of 3 seems to be a good representation for the data for now.\n",
        "\n",
        "Later on i'll try to use embeddings too and see if they add any significant gains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG_KhIK6Faso"
      },
      "source": [
        "#### Let's save this model as the best one for now, and move on to more sophisticated classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbqJgSBf_KW0"
      },
      "source": [
        "scores = {\n",
        "  'Training' : 0.78497,\n",
        "  'Validation' : 0.72373,\n",
        "  'Test' : 0.72621 # Calculated this on another run\n",
        "}\n",
        "best_model_so_far = {\n",
        "    'name' : 'tf-idf bi-gram min-df-3 logit-regressor', # Name of the model\n",
        "    'pipeline_and_params' : (tfidf_lr_pipeline, {}), # A tuple containing a sklearn.Pipeline object and a parameters dict\n",
        "    'scores' : scores, # A dict of scores on each dataset\n",
        "    'time' : 74 # Training time in seconds\n",
        "}"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7g1BI3GOXHs"
      },
      "source": [
        "save_model_pipeline_and_info(best_model_so_far, file_name='best_model_so_far.pickle')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFIMe5oYFu16"
      },
      "source": [
        "## Using better Classifiers on a Bi-Gram Tf-Idf with min_df of 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgP6SKdYLZ8C"
      },
      "source": [
        "old_best_model_so_far = get_model_pipeline_and_info(file_name='best_model_so_far.pickle')\n",
        "best_model_so_far = dict(old_best_model_so_far)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-CDDiDfH0sh"
      },
      "source": [
        "ACCURACY_IMPROVEMENT_MARGIN = 0.03\n",
        "\n",
        "def is_model_fame_worthy(scores):\n",
        "  global best_model_so_far\n",
        "  return scores['Test'] >= (best_model_so_far['scores']['Test'] + ACCURACY_IMPROVEMENT_MARGIN)\n",
        "\n",
        "dict_of_fame = {\n",
        "    old_best_model_so_far['name'] : old_best_model_so_far['scores']\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P7qyGp6qQ4C"
      },
      "source": [
        "def evaluate_model_pipelines(model_pipelines):\n",
        "  for name, (model_pipeline, fit_params) in model_pipelines.items():\n",
        "    print('\\n###########################################################################')\n",
        "    print('# Model type: {}'.format(name))\n",
        "    print('###########################################################################')\n",
        "    \n",
        "    # Train the model\n",
        "    fit_start = perf_counter()\n",
        "    model_pipeline.fit(df_train_for_clf, labels_train_for_clf, **fit_params)\n",
        "    fit_end = perf_counter()\n",
        "\n",
        "    # plot_model_performance(model_history)\n",
        "    \n",
        "    # Calculate the current model's performance and time\n",
        "    # fit time is in seconds\n",
        "    fit_time = fit_end - fit_start\n",
        "    fit_time_hours = int(fit_time / 60 / 60)\n",
        "    fit_time_minutes = int(fit_time / 60 )\n",
        "    fit_time_seconds = int(fit_time % 60)\n",
        "\n",
        "    print(\"Training time: {} hours, {} minutes, {} seconds\".format(fit_time_hours, fit_time_minutes, fit_time_seconds))\n",
        "\n",
        "    train_predictions = model_pipeline.predict(df_train_for_clf)\n",
        "    val_predictions = model_pipeline.predict(df_val_for_clf)\n",
        "    test_predictions = model_pipeline.predict(df_test_for_clf)\n",
        "\n",
        "    scores = {\n",
        "        'Training' : accuracy_score(labels_train_for_clf, train_predictions),\n",
        "        'Validation' : accuracy_score(labels_val_for_clf, val_predictions),\n",
        "        'Test' : accuracy_score(labels_test_for_clf, test_predictions)\n",
        "        }\n",
        "    for dataset_name, score in scores.items():\n",
        "      print (\"%s accuracy: %0.5f \" % (dataset_name, score))\n",
        "\n",
        "    if is_model_fame_worthy(scores):\n",
        "      global best_model_so_far\n",
        "      best_model_so_far = {\n",
        "          'name' : name, # Name of the model\n",
        "          'pipeline_and_params' : (model_pipeline, fit_params), # A tuple containing a sklearn.Pipeline object and a parameters dict\n",
        "          'scores' : scores, # A dict of scores on each dataset\n",
        "          'time' : fit_time # Training time in seconds\n",
        "          }\n",
        "      dict_of_fame[name] = scores\n",
        "\n",
        "  print('\\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
        "  print('@ Best models roundup')\n",
        "  print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
        "  for name, score in dict_of_fame.items():\n",
        "    print(f'Name: {name}\\n Scores: {score}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adovzumzYJpM"
      },
      "source": [
        "#### Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X32-6u55YNum"
      },
      "source": [
        "tfv = TfidfVectorizer(ngram_range=(1, 2), min_df=3)\n",
        "text_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', tfv, TEXTUAL_COLUMNS)\n",
        "    ])\n",
        "\n",
        "# XGB and LightGBM common extra parameters\n",
        "text_preprocessor_for_fancy_clfs = ColumnTransformer(\n",
        "    transformers=[('Text', Pipeline([\n",
        "                                    ('tf-idf', tfv),\n",
        "                                    ('svd', TruncatedSVD(algorithm='randomized', n_components=700))\n",
        "                                    ]),\n",
        "                   TEXTUAL_COLUMNS\n",
        "                   )]\n",
        "    )\n",
        "\n",
        "EVAL_METRIC = 'logloss'\n",
        "\n",
        "# XGBoost and the like expect only numbers\n",
        "# So we'll convert the validation set in advance so we could use it for early stopping\n",
        "text_preprocessor_for_fancy_clfs.fit(df_train_for_clf)\n",
        "df_val_for_gb_clfs = text_preprocessor_for_fancy_clfs.transform(df_val_for_clf)\n",
        "\n",
        "EARLY_STOP_EVAL_SET = [(df_val_for_gb_clfs, labels_val_for_clf)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIljsyDmcjnH"
      },
      "source": [
        "##### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT19hVD3cjnJ"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC(gamma='auto', max_iter=400)\n",
        "fit_params = {}\n",
        "\n",
        "svc_pipeline = Pipeline([('tf_idf', text_preprocessor), \n",
        "                         ('SVM Classifer', svc)])\n",
        "\n",
        "model_pipes_to_evaluate['SVM Classifer with best tf-idf'] = (svc_pipeline, dict(fit_params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ipjhKXaNI5"
      },
      "source": [
        "##### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8Jb44IgaNI7"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rand_f = RandomForestClassifier(n_estimators=1000, n_jobs = -1)\n",
        "fit_params = {}\n",
        "\n",
        "rand_f_pipeline = Pipeline([('tf_idf', text_preprocessor), \n",
        "                         ('rand_forest', rand_f)])\n",
        "\n",
        "model_pipes_to_evaluate['Random Forest with best tf-idf'] = (rand_f_pipeline, dict(fit_params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yVi6Ay-PEXN"
      },
      "source": [
        "##### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oAr_hy5PJnk"
      },
      "source": [
        "xgbc = XGBClassifier(objective = 'binary:logistic',\n",
        "                     n_estimators = 1000,\n",
        "                     learning_rate = 0.01,\n",
        "                     n_jobs = -1,\n",
        "                     tree_method = 'gpu_hist')\n",
        "\n",
        "model_name = 'xgboost'\n",
        "fit_params = {\n",
        "    model_name + '__early_stopping_rounds' : 20,\n",
        "    model_name + '__eval_set' : EARLY_STOP_EVAL_SET, \n",
        "    model_name + '__eval_metric' : EVAL_METRIC,\n",
        "    model_name + '__verbose' : False\n",
        "}\n",
        "\n",
        "xgb_pipeline = Pipeline([('tf_idf', text_preprocessor_for_fancy_clfs), \n",
        "                         (model_name, xgbc)])\n",
        "model_pipes_to_evaluate['XGBRegressor with best tf-idf'] = (xgb_pipeline, dict(fit_params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T02M8vXUg35r"
      },
      "source": [
        "##### LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hh_7GlEg35w"
      },
      "source": [
        "lgbc = LGBMClassifier(n_estimators=1000, n_jobs = -1, objective = 'binary')\n",
        "\n",
        "model_name = 'LightGBM'\n",
        "fit_params = {\n",
        "    model_name + '__early_stopping_rounds' : 20,\n",
        "    model_name + '__eval_set' : EARLY_STOP_EVAL_SET, \n",
        "    model_name + '__eval_metric' : EVAL_METRIC,\n",
        "    model_name + '__verbose' : False\n",
        "}\n",
        "\n",
        "lgbc_pipeline = Pipeline([('tf_idf', text_preprocessor_for_fancy_clfs), \n",
        "                         (model_name, lgbc)])\n",
        "model_pipes_to_evaluate['LightGBM with best tf-idf'] = (lgbc_pipeline, dict(fit_params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Lu3OeYaDUy"
      },
      "source": [
        "#### And for the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJgx7MUJppPH",
        "outputId": "6ecec715-5cd5-4bba-faee-fe97d49bea67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "evaluate_model_pipelines(model_pipes_to_evaluate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "###########################################################################\n",
            "# Model type: SVM Classifer with best tf-idf\n",
            "###########################################################################\n",
            "Training time: 0 hours, 0 minutes, 0 seconds\n",
            "Training accuracy: 0.95000 \n",
            "Validation accuracy: 0.90000 \n",
            "Test accuracy: 0.90000 \n",
            "\n",
            "###########################################################################\n",
            "# Model type: Random Forest with best tf-idf\n",
            "###########################################################################\n",
            "Training time: 0 hours, 0 minutes, 1 seconds\n",
            "Training accuracy: 1.00000 \n",
            "Validation accuracy: 0.90000 \n",
            "Test accuracy: 0.90000 \n",
            "\n",
            "###########################################################################\n",
            "# Model type: XGBRegressor with best tf-idf\n",
            "###########################################################################\n",
            "Training time: 0 hours, 0 minutes, 0 seconds\n",
            "Training accuracy: 0.95000 \n",
            "Validation accuracy: 0.90000 \n",
            "Test accuracy: 0.90000 \n",
            "\n",
            "###########################################################################\n",
            "# Model type: LightGBM with best tf-idf\n",
            "###########################################################################\n",
            "Training time: 0 hours, 0 minutes, 0 seconds\n",
            "Training accuracy: 0.95000 \n",
            "Validation accuracy: 0.90000 \n",
            "Test accuracy: 0.90000 \n",
            "\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@ Best models roundup\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "Name: tf-idf bi-gram min-df-3 logit-regressor\n",
            " Scores: {'Training': 0.78497, 'Validation': 0.72373, 'Test': 0.72621}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWwhd4p6k3gF",
        "outputId": "f3b50276-e4cd-40b9-bcab-fb5f6b48534d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "best_model_so_far"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'SVM Classifer with best tf-idf',\n",
              " 'pipeline_and_params': (Pipeline(memory=None,\n",
              "           steps=[('tf_idf',\n",
              "                   ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                     sparse_threshold=0.3,\n",
              "                                     transformer_weights=None,\n",
              "                                     transformers=[('text',\n",
              "                                                    TfidfVectorizer(analyzer='word',\n",
              "                                                                    binary=False,\n",
              "                                                                    decode_error='strict',\n",
              "                                                                    dtype=<class 'numpy.float64'>,\n",
              "                                                                    encoding='utf-8',\n",
              "                                                                    input='content',\n",
              "                                                                    lowercase=True,\n",
              "                                                                    max_df=1.0,\n",
              "                                                                    max_features=None,\n",
              "                                                                    min_df=3,\n",
              "                                                                    ngram_range=(1,...\n",
              "                                                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                                                    tokenizer=None,\n",
              "                                                                    use_idf=True,\n",
              "                                                                    vocabulary=None),\n",
              "                                                    'processed comment')],\n",
              "                                     verbose=False)),\n",
              "                  ('SVM Classifer',\n",
              "                   SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                       coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                       gamma='auto', kernel='rbf', max_iter=400,\n",
              "                       probability=False, random_state=None, shrinking=True,\n",
              "                       tol=0.001, verbose=False))],\n",
              "           verbose=False), {}),\n",
              " 'scores': {'Test': 0.9, 'Training': 0.95, 'Validation': 0.9},\n",
              " 'time': 0.010725947000537417}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    }
  ]
}